\documentclass[main.tex]{subfiles}

\begin{document}
    \chapter{Infinte Dimensional Vector Space Business}

    \section{Extra Results About Finite Dimensional Vector Spaces}
    Here, we will state a theorem that we looked at in the previous class. 
    \begin{thrm}{}{}
        Let $V$ be a finite-dimensional vector space over $F$. Then $B$ is a basis of $V$ if and only if each $v\in V$ can be uniquely represented as a linear combination of vectors from $B$.
    \end{thrm}
    \begin{proof}
        \textbf{Proof of $(\implies)$:} let $B = \{v_1, v_2, ..., v_n\}$ be a basis of $V$ and let $v\in V$ be an arbitrary member of $V$. Now, since $\Span B = V$ so we have $v\in \Span B$. So $v$ is a linear combination of vectors from $B$. Let $v = a_1v_1 + a_2v_2 + \cdots + a_nv_n = b_1v_1 + b_2v_2 + \cdots + b_nv_n$ where $a_i,b_i\in F$ for all $i\in \{1, ..., n\}$. Now, if we subtract these two representations, we get 
        \begin{equation}
            (a_1 - b_1)v_1 + (a_2 - b_2)v_2 + \cdots + (a_n - b_n)v_n = 0
        \end{equation}
        But since we have that $v_1, v_2, ..., v_n$ are linearly independent, we must have $a_i - b_i = 0$. It follows that $a_i = b_i$ and the representations are unique. \bigbreak 

        \noindent \textbf{Proof of $(\impliedby)$:} since each $v\in V$ is a unique linear combination of vectors from $B$, it follows that $\Span B = V$. Now we have to show that $B$ is linearly independent. Assume that $a_1v_1 + \cdots + a_nv_n = 0$ where $a_i, ..., a_n \in F$. Note that $0 = 0v_1 + \cdots + 0v_n$. By uniqueness of the representations, we must have $a_i = 0$ for all $i\in \{1, ..., n\}$. So $B$ is linearly independent. Since $B$ is linearly independent and spans $V$, we have that $B$ is a basis of $V$.
    \end{proof}

    Here is another theorem that one should look at on their own time. 
    \begin{thrm}{}{}
        If a vector space $V$ is generated by a finite set $S$, then $S$ contains a basis of $V$. Consequently, $V$ has a finite basis. 
    \end{thrm}
    \begin{proof}
        To be done as take home reading
    \end{proof}

    From the theorems above, we state and define the following corollary, that we have used in Math 250 before.
    \begin{cor}{}{}
        Let $V$ be a finite dimensional vector space of dimension $n$ over $F$. Then
        \begin{enumerate}
            \item Any linearly independent set of vectors has less than or equal to $n$ elements. Furthermore, if there are exactly $n$ elements, the set is a basis of $V$.

            \item Any generating set of $V$ has at least $n$ vectors. If there are exactly $n$ vectors, then the generating set is a basis of $V$.
        \end{enumerate}
    \end{cor}
    \begin{proof}
        More take home reading
    \end{proof}

    Now, we will state and prove this (very important) theorem. 
    \begin{thrm}{}{}
        Let $V$ be a finite dimensional vector space over $F$ and let $W$ be a subspace of $V$. Then $\Dim W \leq \Dim V$. Furthermore, if $\Dim W = \Dim V$, then $W = V$.
    \end{thrm}
    \begin{proof}
        Let $\Dim V = n$ and let $B_W$ be a basis for $W$. Since $B_W$ is a set of linearly independent vectors from $V$, we have that $B_W$ is finite and $|B_W| \leq n$. By definition of dimension, we have $\Dim W \leq n = \Dim V$. \par 

        Now assume $\Dim W = \Dim V$. Let $B_W$ be a basis of $W$. Then $B_W$ is a linearly independent set of vectors from $V$ with cardinality equal to $\Dim V$. Then by the corollary, we have that $B_W$ is a basis of $V$. So $\Span B_W = V$. Since $\Span B_W = W$, we must have that $W = V$. 
    \end{proof}

    Here is a corollary to this theorem. 
    \begin{cor}{}{}
        Let $V$ be a finite dimensional vector space over $F$ and let $W$ be a subspace of $V$. Then any basis of $W$ can be extended to a basis of $V$.
    \end{cor}

    \section{Maximal Linearly Independent Sets}
    Here are some definitions that we will use. 
    \begin{defn}{Maximal Set}{}
        Let $\mathcal{F}$ be a collection of sets. Then a set $M\in \mathcal{F}$ is said to be maximal (with respect to the set inclusion order) if $M$ is contained in no member of $\mathcal{F}$ other than $M$ itself.
    \end{defn}
    \begin{example}{}{}
        Take $A$ to be a nonempty set. Take $\mathcal{F} = \mathcal{P}(A)$ (the power set of $A$, which is the collection of all subsets of $A$). Then $A$ is a maximal element of $\mathcal{F}$ (since $A$ is contained in no subset of $A$ other than itself). \bigbreak 

        Here is a non-example: let $A$ be an infinite set and take $\mathcal{F} = \{S \subseteq A : S \text{ is finite}\}$. Note that this set does not have a maximal elements. Assume for contradiction that $M\in \mathcal{F}$ was a maximal element. Then $M$ must be finite. So there exists $a\in A\setminus M$. Then we have that $M\cup \{a\} \in \mathcal{F}$, since it is a finite subset of $A$. But this contradicts that $M$ is maximal since this set contains $M$. So there is no maximal element in this set.
    \end{example}

    Now we will introduce the concept of a chain, which was discussed in Math 300H. 
    \begin{defn}{Chain}{}
        A family of sets $\mathcal{F}$ is called a chain (or nest or tower, with respect to set inclusion) if given any two $S_1, S_2 \in \mathcal{F}$ either $S_1 \subseteq S_2$ or $S_2 \subseteq S_1$
    \end{defn}

    Now, we will discuss a very important theorem in this area of math. We will assume this theorem without proof. 
    \begin{thrm}{Zorn's Lemma}{}
        Let $\mathcal{F}$ be a family of sets. If for every chain $\mathcal{C} \subseteq \mathcal{F}$ there exists a $U\in \mathcal{F}$ such that $S \subseteq U$ for all $S\in \mathcal{C}$, then $\mathcal{F}$ has a maximal element.
    \end{thrm}
    Here is a theorem that will utilize these concepts. 
    \begin{thrm}{}{}
        Let $V$ be a vector space and $V = \Span S$. If $B$ is a maximal linearly independent subset of $S$, then $B$ is a basis of $V$. 
    \end{thrm}
    \begin{proof}
        The hypothesis of the theorem states that $B$ is linearly independent. So we will only need to prove that $S \subseteq \Span B$. If not, then there exists $s\in S \setminus \Span B$. So $s\not\in \Span B$ which implies that $B\cup \{s\}$ is a linearly independent set which contradicts the assumption that $B$ is a maximal linearly independent set. Thus, we have $S \subseteq \Span B$ and consequently, we have $V = \Span B$. 
    \end{proof}

    Here is the theorem that will utilize Zorn's lemma. 
    \begin{thrm}{}{}
        Let $S$ be a linearly independent subset of vector space $V$. Then there exists a maximal linearly independent subset of $V$ that contains $S$. 
    \end{thrm}
    \begin{proof}
        Let $\mathcal{F}$ be the family that contains all linearly independent subsets of $V$ containing $S$. Let $\mathcal{C}$ be a chain such that $\mathcal{C} \subseteq \mathbf{F}$. Now consider $U = \bigcup_{C\in \mathcal{C}} C$. Note that since $S\subseteq \mathcal{C}$for all $C\in \mathcal{C}$, we have $S \subseteq U$. So $U$ is an upper bound for an arbitrary chain. Now, we must show $U\in \mathcal{F}$, which involves showing that $U$ is linearly independent. Let $a_1u_1 + a_2u_2 + \cdots + a_nu_n = 0$ for $a_i \in F$ and $u_i \in U$ for all $i \in \{1, ..., n\}$. Since $u_i \in U$ we have $u_i \in C_j$ for some $C_j \in \mathcal{C}$. Since $\mathcal{C}$ is a chain, there exists a $k_0 \in \{1, ..., n\}$ such that $C_i \subseteq C_{k_0}$ for all $i\in \{1, ..., n\}$. (prove this as an exercise using induction). Since $C_{k_0}$ is linearly independent we get $a_1 = a_2 = \cdots = a_n = 0$. Since $\{u_1, u_2, ..., u_n\}$ is arbitrary for $U$, we have $U$ is linearly independent. So $U\in \mathcal{F}$ and $U$ is an upper bound of $\mathcal{C}$. \bigbreak 

        By Zorn's lemma, we have that $\mathcal{F}$ has a maximal element and so there exists a maximal element linearly independent subset of $V$ that contains $S$. By the previous theorem, we have that this maximal member is a basis of $V$. 
    \end{proof}
    Here is a corollary to this theorem, that will establish a well known fact that we knew for finite dimensional vector spaces. 

    \begin{cor}{}{}
        Every vector space has a basis.
    \end{cor}
\end{document}
\documentclass[main.tex]{subfiles}

\begin{document}
    \chapter{Linear Independence, Basis, and Dimension}

    \section{Linear Independence \& Dependence}
    We define the concept of linearly independent and linearly dependent below.

    \begin{defn}{Linear Independence}{}
        Let $V$ be a vector space over $F$. Let $S$ be a collection of vectors from $V$. Then the set $S$ is said to be linearly independent if $a_1x_1 + a_2x_2 + \cdots + a_nx_n = 0$ for all finite subcollections $\{x_1, \dots, x_n\}$ in $S$ and scalars $a_1, a_2, ..., a_n \in F$, then $a_1 = a_2 = \cdots = a_n = 0$. 
    \end{defn}

    \begin{defn}{Linear Dependence}{}
        If a set $S\subseteq V$ is not linearly independent, then we say that is linearly dependent. 
    \end{defn}

    \begin{example}{}{}
        Consider the vector space $\R^n$ over the field $\R$. Then we have that 
        \begin{equation*}
            S_m = \{e_1, e_2, \dots, e_n \} \qquad m \leq n
        \end{equation*}
        Then the set $S_m$ is linearly independent. Note that $e_i$ is the vector of all 0s except for the $i$-th component, which has a value of 1. \bigbreak 

        Now consider the set $S = \{e_1, e_2, e_1 + e_2\}$. This set is not linearly independent since we have that $(-1)e_1 + (-1)e_2 + 1(e_1 + e_2) = 0$. Thus, there is a non-trivial linear combination of the vectors in $S$ that equals 0. 
    \end{example}

    \begin{example}{}{}
        Consider the vector space $P(\R)$ which is all polynomials with real number coefficients. Then consider the set $S = \{x^i : i \in \N \cup \{0\}\}$. This set is linearly independent in $P(\R)$. This is an infinite set that is linearly independent. \bigbreak 

        Here is another example of an infinite set that is linearly independent. Consider the vector space of all sequences with real valued entries. Then $T = \{e_i : i \in \N\}$ is linearly independent.
    \end{example}

    \section{Bases}
    With the notion of linear independence established, we now move on to the definition of a basis. 
    \begin{defn}{Basis}{}
        Let $V$ be a vector space over $F$. Let $B$ be a set of vectors from $V$. Then the set $B$ is a basis of $V$ if $B$ is linearly independent and $\Span B = V$. 
    \end{defn}
    A basis is essentially just a linearly independent spanning set for a subspace. Here are some examples. 

    \begin{example}{}{}
        The set $S_n = \{e_1, e_2, \dots, e_n\}$ is a basis for the subspace $\R^n$. Note that the basis for this set is finite. \bigbreak 

        The set $S$ defined in a previous example is a basis for $P(\R)$. Consider $f\in P(\R)$. Then $f = a_0 + a_1x + a_2x^2 + \cdots + a_nx^n$ for $a_1, a_2, \dots, a_n \in \R$. So we have that $f\in \Span S$. 
    \end{example}

    \begin{example}{}{}
        Consider the set $S = \{e_i : i\in \N\}$ defined in a previous example. Even though the set is linearly independent, this set is not a basis. For a counterexample, consider the sequence $\mathbf{1} : \N \to \R$ which is given by $\mathbf{1}(n) = 1$ for all $n\in \N$. We have that $\mathbf{1} \not\in \Span S$. 
    \end{example}
    Now we will prove some important theorems regarding basis. The first one will lead to the concept of dimension later on. We formally introduce the terminology of finite-dimensional vector spaces.

    \begin{defn}{Finite Dimensional Vector Space}{}
        Let $V$ be a vector space over $F$. Then we say $V$ is finite dimensional if any basis for $V$ is finite. 
    \end{defn}
    The definition will also lead to the important result that will be proved in this theorem. 
    \begin{thrm}{}{}
        Let $V$ be a finite dimensional vector space over $F$. Let $B = \{b_1, b_2, \dots, b_n\}$ be a basis for $V$. Then if $S \subseteq V$ has $m$ vectors where $m > n$, then $S$ is linearly dependent. 
    \end{thrm}
    \begin{proof}
        Let $S = \{x_1, x_2, \dots, x_m\}$. Then to show $S$ is linearly dependent, we must find scalars $c_1, c_2, \dots, c_m \in F$, not all 0, such that $c_1x_1 + \cdots c_mx_m = 0$. Then since $B$ is a basis for $V$, we have 
        \begin{equation}
            \begin{aligned}
                x_1 &= \sum_{j = 1}^n \alpha_{1j}b_j \qquad \text{for $\alpha_{1j}\in F$, $j = 1, ..., n$} \\
                x_2 &= \sum_{j = 1}^n \alpha_{2j}b_j \qquad \text{for $\alpha_{2j}\in F$, $j = 1, ..., n$} \\
                \vdots \\
                x_m &= \sum_{j = 1}^n \alpha_{mj}b_j \qquad \text{for $\alpha_{mj}\in F$, $j = 1, ..., n$} \\
            \end{aligned}  
        \end{equation}
        Then we have the original equation is 
        \begin{equation}
            c_1\sum_{j=1}^n \alpha_{1j}b_j + c_2 \sum_{j = 2}^n \alpha_{2j}b_j + \cdots c_m \sum_{j = 1}^n \alpha_{mj}b_j = 0
        \end{equation}
        Then we can collect the coefficients in front of each $b_j$. This gives us that 
        \begin{equation}
            (c_1\alpha_{11} + c_2\alpha_{21} + \cdots + c_m\alpha_{m1})b_1 + \cdots + ()b_j = 0
        \end{equation}
        Then since $B$ was a basis, we have that it is linearly independent. Thus, the individual coefficients must be 0. This gives us that 
        \begin{equation}
            \begin{aligned}
                \sum_{i = 1}^m \alpha_{i1}c_i &= 0 \\
                \sum_{i = 1}^m \alpha_{i2}c_i &= 0 \\
                \vdots \\
                \sum_{i = 1}^m \alpha_{in}c_i &= 0
            \end{aligned}
        \end{equation}
        Finish proof in PSS session: from elementary linear algebra, we know that since $m>n$, the null space of this system is non-trivial. Pick any nonzero vector from the null space, which will be a set of scalars that will suffice. 
    \end{proof}
    From the proof of the statement above, we have the following corollary. 
    \begin{cor}{}{}
        If $V$ is a finite dimensional vector space over $F$. Then, any two bases of $V$ must have the same number of vectors. 
    \end{cor}

    Here are some other propositions that were introduced in an elementary linear algebra course. Note that we prove these theorems in the next class and use some results from the next class to aid in the proofs.
    \begin{prop}{}{}
        Let $V$ be a finite dimensional vector space and $S\subseteq V$ is linearly independent. Then, $V$ has a basis $B$ and $B \supseteq S$.
    \end{prop}
    \begin{proof}
        Assume the dimension of $V$ is equal to $n$. We have two cases to consider: either $|S| = n$ or $|S| < n$. If $|S| = n$, then $S$ is a basis for $V$ and the result is proved. So assume that $|S| < n$. Then we must have that $S$ does not span $V$ from the theorem above. Then using Theorem 1.7 (in the book), there exists a $v\in V$ that is not in the span of $S$. Thus, we have that $S\cup \{v\}$ is a linearly independent set with size $|S| + 1$. This process can repeat until we arrive at a generating set for $V$, which (by the previous problem), will occur when there are $n$ vectors. 
    \end{proof}

    \begin{prop}{}{}
        Let $V$ be a finite dimensional vector space over $F$. Let $S$ be a subset of $V$ such that $\Span S = V$. Then $S$ contains a basis of $V$. 
    \end{prop}
    \begin{proof}
        Since $V$ is finite dimensional, then it must have a finite basis that we'll call $B = \{b_1, b_2, \dots, b_n\}$. Now let $\Sigma = \{A \subseteq S : A \text{ is finite and linearly independent}\}$. Note that $V$ cannot have an infinite linearly independent subset. This is because if this were the case, then all subsets of $V$ also be linearly independent. But then, we can find a linearly independent subset of size $n+1$, which contradicts PSS2. \par 
        
        Now for all $A\in \Sigma$, we must have $|A| \leq n$. We claim that there exists $A\in \Sigma$ such that $|A| = n$. Let $\N_\Sigma = \{|A| : A \in \Sigma\}$. Note that for all $m\in \N_|Sigma$, we must have $m\leq n$. So $n$ is an upper bound of $\N_\Sigma$. So $\N_\Sigma$ has a maximum element since it is a finite subset of $\N$. Note that if $|A| < n$, then $\Span A \neq V$. So there exists a $v\in S$ such that $A\cup \{v\}$ is linearly independent (otherwise it would contradict the fact that $\Span S = V$). This contradicts the assumption that $|A|$ is a maximum and so there is a set of size $n$ that follows. \par 
        
        Now we claim that the set $A$ from the previous claim is a basis for $V$. We know that $A$ must be linearly independent, so we must have that $\Span A \neq V$. So there exists $v\in V\setminus \Span A$ so $A$ is linearly independent and $v\not\in \Span A$. By theorem 1.7, we have that $A\cup \{v\}$ is linearly independent. But since $A\cup \{v\}$ has $n+1$ members, this contradicts PSS2. So $A$ is a basis of $V$. \par 
        
        So since $A\subseteq S$ we have proved that $S$ contains a basis of $V$.
    \end{proof}
    The two propositions above lead to an important corollary. 

    \begin{cor}{}{}
        Every non-trivial vector space has a basis.
    \end{cor}
    \begin{proof}
        Let $x\in V$ such that $x \neq 0$. Then the set $\{x\}$ is linearly independent. By proposition 1.1, we have that $\{x\}$ is contained in a basis of $V$.
    \end{proof}

    \section{End of Class Problems}
    The proofs of the propositions are the in-class problems. The other problem is to construct a basis for $V_S(\R)$.
\end{document}